	''There is a practical problem out there, and they have come up with a very simple solution,'' said Albert-Laszlo Barabasi, a professor of physics at the University of Notre Dame and author of ''Linked: The New Science of Networks'' (Perseus, 2002). ''This is one of those things that in retrospect makes people say, 'It's so simple, why didn't I think of that?'''
	In large-scale parallel processing, several computers are joined through communications networks to create a single giant supercomputer. Of course, the processing speed that such a system offers adds to its complexity.
	While doing postdoctoral work at the University of Florida, another physicist, Gyorgy Korniss, was frustrated by the inability of networked computers to stay synchronized. With some parallel systems, that is not an issue; the computers analyzing radio telescope data for signs of extraterrestrial intelligence in the SETI@home project, for example, all do their work independently.
	But parallel networks of the kind that Dr. Korniss was working with, which are used to develop models of complex systems like the spread of disease or the growth of crystals, generally require that every computer in the network finish work on one stage of a problem before a new round of calculations can begin.
